{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3aa0b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•œêµ­ì–´ -> ì¼ë³¸ì–´ : ['ã“ã‚“ã«ã¡ã¯ã€ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€å…ƒæ°—ã§ã™ã€ã¾ãŸä¼šã„ã¾ã—ã‚‡ã†ã€‚']\n",
      "í•œêµ­ì–´ -> ì˜ì–´ : ['Hello, thank you, itâ€™s good, weâ€™ll see you again.']\n",
      "ì¼ë³¸ì–´ -> í•œêµ­ì–´ : ['ì•ˆë…•í•˜ì„¸ìš”, ê³ ë§ˆì›Œìš”, ì˜ ì§€ë‚´ê³  ë‹¤ì‹œ ë§Œë‚˜ê² ìŠµë‹ˆë‹¤.']\n",
      "ì˜ì–´ -> í•œêµ­ì–´ : ['ì•ˆë…•í•˜ì„¸ìš”, ê°ì‚¬í•©ë‹ˆë‹¤, ì¢‹ì€ ì¼ì…ë‹ˆë‹¤, ìš°ë¦¬ëŠ” ë‹¤ì‹œ ë‹¹ì‹ ì„ ë³¼ ê²ƒì…ë‹ˆë‹¤.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "ko_text = \"ì•ˆë…•í•˜ì„¸ìš”, ê°ì‚¬í•´ìš”, ì˜ ìˆì–´ìš”, ë‹¤ì‹œ ë§Œë‚˜ìš”\"\n",
    "\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "tokenizer.src_lang = \"ko\"\n",
    "encoded_hi = tokenizer(ko_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"ja\"))\n",
    "kor_to_jap = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "print('í•œêµ­ì–´ -> ì¼ë³¸ì–´ :', kor_to_jap)\n",
    "\n",
    "\n",
    "tokenizer.src_lang = \"ko\"\n",
    "encoded_zh = tokenizer(ko_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "kor_to_eng = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "print('í•œêµ­ì–´ -> ì˜ì–´ :', kor_to_eng)\n",
    "\n",
    "tokenizer.src_lang = \"ja\"\n",
    "encoded_hi = tokenizer(kor_to_jap, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"ko\"))\n",
    "kor_to_jap = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "print('ì¼ë³¸ì–´ -> í•œêµ­ì–´ :', kor_to_jap)\n",
    "\n",
    "tokenizer.src_lang = \"en\"\n",
    "encoded_hi = tokenizer(kor_to_eng, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"ko\"))\n",
    "kor_to_jap = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "print('ì˜ì–´ -> í•œêµ­ì–´ :', kor_to_jap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18ed7a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://4b533e89781be58a0a.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4b533e89781be58a0a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ğŸ“– ë‹¤êµ­ì–´ ë²ˆì—­ê¸° (Gradio + M2M100)\n",
    "- transformersì˜ facebook/m2m100_418M ëª¨ë¸ ì‚¬ìš©\n",
    "- Gradioë¡œ ì›¹ UI êµ¬ì„±\n",
    "\"\"\"\n",
    "\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import gradio as gr\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\"\"\"\n",
    "    model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "    tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# 1. ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model, tokenizer = load_model()\n",
    "\n",
    "# 2. ì§€ì› ì–¸ì–´ ë§¤í•‘\n",
    "LANGS = {\n",
    "    \"í•œêµ­ì–´\": \"ko\",\n",
    "    \"ì˜ì–´\": \"en\",\n",
    "    \"ì¼ë³¸ì–´\": \"ja\",\n",
    "    \"ì¤‘êµ­ì–´(ê°„ì²´)\": \"zh\",\n",
    "    \"ìŠ¤í˜ì¸ì–´\": \"es\",\n",
    "    \"í”„ë‘ìŠ¤ì–´\": \"fr\"\n",
    "}\n",
    "\n",
    "# 3. ë²ˆì—­ í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "def translate(text: str, src_lang: str, tgt_lang: str) -> str:\n",
    "    \"\"\"\n",
    "    text: ë²ˆì—­í•  ë¬¸ì¥\n",
    "    src_lang: ì…ë ¥ ì–¸ì–´ ì½”ë“œ (ex: 'ko')\n",
    "    tgt_lang: ì¶œë ¥ ì–¸ì–´ ì½”ë“œ (ex: 'en')\n",
    "    \"\"\"\n",
    "    tokenizer.src_lang = src_lang\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "    generated = model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=tokenizer.get_lang_id(tgt_lang)\n",
    "    )\n",
    "    return tokenizer.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "\n",
    "# 4. Gradio ì¸í„°í˜ì´ìŠ¤\n",
    "\n",
    "def build_interface():\n",
    "    # ì…ë ¥ ì»´í¬ë„ŒíŠ¸ ì„¤ì •\n",
    "    txt = gr.Textbox(lines=3, placeholder=\"ë²ˆì—­í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”\", label=\"ì›ë¬¸\")\n",
    "    src = gr.Dropdown(choices=list(LANGS.keys()), label=\"ì›ë¬¸ ì–¸ì–´\", value=\"í•œêµ­ì–´\")\n",
    "    tgt = gr.Dropdown(choices=list(LANGS.keys()), label=\"ëª©í‘œ ì–¸ì–´\", value=\"ì˜ì–´\")\n",
    "    output = gr.Textbox(label=\"ë²ˆì—­ ê²°ê³¼\")\n",
    "\n",
    "    # ì¸í„°í˜ì´ìŠ¤ ìƒì„±\n",
    "    iface = gr.Interface(\n",
    "        fn=lambda text, s, t: translate(text, LANGS[s], LANGS[t]),\n",
    "        inputs=[txt, src, tgt],\n",
    "        outputs=output,\n",
    "        title=\"ğŸ“– ë‹¤êµ­ì–´ ë²ˆì—­ê¸°\",\n",
    "        description=\"facebook/m2m100_418M ëª¨ë¸ ê¸°ë°˜ ê°„í¸ ë‹¤êµ­ì–´ ë²ˆì—­ ì„œë¹„ìŠ¤\"\n",
    "    )\n",
    "    return iface\n",
    "\n",
    "# 5. ë°ëª¨ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    demo = build_interface()\n",
    "    # ë¡œì»¬ ì‹¤í–‰ ë° ì™¸ë¶€ ê³µìœ  í™œì„±í™”\n",
    "    demo.launch(share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a6659e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22838e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fbe5f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\miniforge3\\envs\\ai_seving\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--microsoft--speecht5_tts. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Admin\\miniforge3\\envs\\ai_seving\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--microsoft--speecht5_hifigan. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Admin\\miniforge3\\envs\\ai_seving\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\datasets--Matthijs--cmu-arctic-xvectors. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7931/7931 [00:00<00:00, 56489.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from datasets import load_dataset\n",
    "\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "inputs = processor(text=\"Hello, my dog is cute.\", return_tensors=\"pt\")\n",
    "\n",
    "# load xvector containing speaker's voice characteristics from a dataset\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "\n",
    "sf.write(\"speech.wav\", speech.numpy(), samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31000746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://7f7fecc1c5668ee1ba.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7f7fecc1c5668ee1ba.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\miniforge3\\envs\\ai_seving\\Lib\\site-packages\\gradio\\processing_utils.py:753: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ğŸ“– ë‹¤êµ­ì–´ ë²ˆì—­ê¸° + ì½ì–´ì£¼ê¸° ê¸°ëŠ¥ (Gradio + M2M100 + SpeechT5)\n",
    "- transformersì˜ facebook/m2m100_418M ëª¨ë¸ë¡œ ë²ˆì—­\n",
    "- microsoft/speecht5_tts ëª¨ë¸ë¡œ ì˜ì–´ TTS\n",
    "- Gradioë¡œ ì›¹ UI êµ¬ì„±\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import gradio as gr\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”\n",
    "model_mt = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer_mt = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "processor_tts = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model_tts = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder_tts = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "# 2. ì§€ì› ì–¸ì–´ ë§¤í•‘\n",
    "LANGS = {\n",
    "    \"í•œêµ­ì–´\": \"ko\",\n",
    "    \"ì˜ì–´\": \"en\",\n",
    "    \"ì¼ë³¸ì–´\": \"ja\",\n",
    "    \"ì¤‘êµ­ì–´(ê°„ì²´)\": \"zh\",\n",
    "    \"ìŠ¤í˜ì¸ì–´\": \"es\",\n",
    "    \"í”„ë‘ìŠ¤ì–´\": \"fr\"\n",
    "}\n",
    "\n",
    "# 3. ìŠ¤í”¼ì»¤ ì„ë² ë”© ë¡œë“œ (ì˜ˆì‹œìš©)\n",
    "emb_ds = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embedding = torch.tensor(emb_ds[7306][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "# 4. ë²ˆì—­ ë° TTS í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "def translate_and_tts(text: str, src_lang_label: str, tgt_lang_label: str):\n",
    "    # ë ˆì´ë¸”ì„ ì–¸ì–´ ì½”ë“œë¡œ ë³€í™˜\n",
    "    src_code = LANGS[src_lang_label]\n",
    "    tgt_code = LANGS[tgt_lang_label]\n",
    "\n",
    "    # ë²ˆì—­\n",
    "    tokenizer_mt.src_lang = src_code\n",
    "    encoded = tokenizer_mt(text, return_tensors=\"pt\")\n",
    "    gen = model_mt.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=tokenizer_mt.get_lang_id(tgt_code)\n",
    "    )\n",
    "    translated = tokenizer_mt.batch_decode(gen, skip_special_tokens=True)[0]\n",
    "\n",
    "    # ì˜ì–´ì¼ ê²½ìš° TTS ìƒì„±\n",
    "    audio_data = None\n",
    "    if tgt_code == \"en\":\n",
    "        inputs_tts = processor_tts(text=translated, return_tensors=\"pt\")\n",
    "        speech = model_tts.generate_speech(\n",
    "            inputs_tts[\"input_ids\"], speaker_embedding, vocoder=vocoder_tts\n",
    "        )\n",
    "        audio_data = speech.cpu().numpy()\n",
    "\n",
    "    # ë°˜í™˜ í˜•ì‹: (ë²ˆì—­ë¬¸, (ìƒ˜í”Œë ˆì´íŠ¸, ì˜¤ë””ì˜¤) or None)\n",
    "    return translated, (16000, audio_data) if audio_data is not None else None\n",
    "\n",
    "# 5. Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì¶•\n",
    "\n",
    "def build_interface():\n",
    "    txt = gr.Textbox(lines=3, placeholder=\"ë²ˆì—­í•  í…ìŠ¤íŠ¸ ì…ë ¥\", label=\"ì›ë¬¸\")\n",
    "    src = gr.Dropdown(choices=list(LANGS.keys()), label=\"ì›ë¬¸ ì–¸ì–´\", value=\"í•œêµ­ì–´\")\n",
    "    tgt = gr.Dropdown(choices=list(LANGS.keys()), label=\"ëª©í‘œ ì–¸ì–´\", value=\"ì˜ì–´\")\n",
    "    out_txt = gr.Textbox(label=\"ë²ˆì—­ ê²°ê³¼\")\n",
    "    out_audio = gr.Audio(label=\"ì½ì–´ì£¼ê¸° (ì˜ì–´)\")\n",
    "\n",
    "    iface = gr.Interface(\n",
    "        fn=translate_and_tts,\n",
    "        inputs=[txt, src, tgt],\n",
    "        outputs=[out_txt, out_audio],\n",
    "        title=\"ğŸ“– ë‹¤êµ­ì–´ ë²ˆì—­ê¸° + ì½ì–´ì£¼ê¸°\",\n",
    "        description=\"ë²ˆì—­ ë° ì˜ì–´ì— í•œí•´ TTS(ì½ì–´ì£¼ê¸°) ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\"\n",
    "    )\n",
    "    return iface\n",
    "\n",
    "# 6. ë°ëª¨ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    demo = build_interface()\n",
    "    demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb15a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe4a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60122b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b8d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5633736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1ff42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a18d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79918a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98534c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd524c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edacf8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
